{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: **Hyperparameter Tuning for AtliQâ€™s Fashion Item Classifier**\n",
        "\n",
        "### AtliQ Fashion wants to develop a neural network to classify fashion items using the FashionMNIST dataset. Your task is to optimize the neural network's performance by fine-tuning its hyperparameters. We will be using **FashionMNIST** dataset but since the dataset is large, we will work with only a subset to ensure that the solution is computationally feasible.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* transforms.Compose (PyTorch): [Link](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)\n",
        "* Optuna (Hyperparameter Optimization Framework) [Link](https://optuna.readthedocs.io/en/stable/)"
      ],
      "metadata": {
        "id": "HS_V6StcOXOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "5mz3pMHtjJ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import optuna\n",
        "import random\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "Nv9L_r8yOjoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "65TpZ6z3YVcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Overview**\n",
        "\n",
        "* Dataset: FashionMNIST\n",
        "* Classes: 10 (e.g., T-shirts, trousers, shoes)\n",
        "* Training Images: Subset of 10,000 (randomly sampled from 60,000)\n",
        "* Test Images: Subset of 2,000 (randomly sampled from 10,000)"
      ],
      "metadata": {
        "id": "nBrkVANZOnea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MJiS4LzoYWh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1**: Load and Sample the Dataset\n",
        "\n",
        "* Load the FashionMNIST dataset using torchvision.datasets.\n",
        "* Sample 10,000 images for training and 2,000 images for testing.\n",
        "* Normalize the pixel values to the range [-1, 1].\n",
        "* Create PyTorch DataLoaders for the training and test sets."
      ],
      "metadata": {
        "id": "caIjvjTMOuKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ESYRM7JK-UW"
      },
      "outputs": [],
      "source": [
        "# Transform: Normalize and convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Centers the pixel values around 0 and scales them to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load FashionMNIST dataset\n",
        "\n",
        "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_dataset = # Code Here\n",
        "\n",
        "\n",
        "\n",
        "# Sample the datset\n",
        "\n",
        "train_subset_size = # Code Here\n",
        "test_subset_size = # Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S9NJ-ejsYYU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2**: Create Dataloaders\n",
        "\n",
        "* batch size = 32\n"
      ],
      "metadata": {
        "id": "wbAUaSPmSfsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset, _ = random_split(dataset, [train_subset_size, len(dataset) - train_subset_size])\n",
        "test_subset, _ = random_split(test_dataset, [test_subset_size, len(test_dataset) - test_subset_size])"
      ],
      "metadata": {
        "id": "xPPkBtqbacNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = # Code Here\n",
        "test_loader = # Code Here"
      ],
      "metadata": {
        "id": "Hz00fg0sSqT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training data size: {len(train_subset)}\")\n",
        "print(f\"Testing data size: {len(test_subset)}\")"
      ],
      "metadata": {
        "id": "oq15WEfhS1HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1dSLUp9EYaG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3**: Define the Neural Network\n",
        "\n",
        "* Create a fully connected feed-forward neural network (no CNN).\n",
        "\n",
        "Structure:\n",
        "* Input layer: 784 neurons (28x28 image flattened).\n",
        "* 1st hidden layer: 128 neurons with ReLU activation.\n",
        "* 2nd hidden layer: 64 neurons with ReLU activation.\n",
        "* Output layer: 10 neurons (one for each class) with Softmax activation.\n",
        "\n",
        "Use `nn.Sequential`"
      ],
      "metadata": {
        "id": "bGRSPjG0StuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionNN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Flatten the input tensor\n",
        "\n",
        "            # Input layer (784)\n",
        "\n",
        "            # Activation\n",
        "\n",
        "            # Hidden layer 1\n",
        "\n",
        "            # Activation\n",
        "\n",
        "            # Output layer (10 classes)\n",
        "\n",
        "            # Softmax for probabilities\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = # Code Here\n",
        "print(model)\n",
        "\n",
        "loss_fn = # Code Here"
      ],
      "metadata": {
        "id": "QJYOKAZmTpkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UJmkGAINYb0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Train the Base Model\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Set the following base hyperparameters:\n",
        "* Loss function: Cross Entropy Loss\n",
        "* Learning rate: 0.01\n",
        "* Batch size: 32\n",
        "* Optimizer: SGD\n",
        "* Epochs: 100\n",
        "\n",
        "Train the model and record the training/validation accuracy and loss.\n"
      ],
      "metadata": {
        "id": "PNO5O1W_UI7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "loss_function = # Code Here\n",
        "optimizer = # Code Here\n",
        "\n",
        "# Training loop\n",
        "num_epochs =\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    model.train()  # Set model to training mode\n",
        "    for images, labels in train_loader:\n",
        "        # Zero gradients\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = # Code Here\n",
        "        loss = # Code Here\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Append the training loss\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "Mb5gFuiUUTDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qAu40Az5itQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: Perform Hyperparameter Tuning\n",
        "Instructions:\n",
        "\n",
        "**Grid Search:**\n",
        "\n",
        "Hyperparameters:\n",
        "* Learning rate: [0.001, 0.01, 0.1]\n",
        "* Batch size: [32, 64]\n",
        "* Evaluate all combinations systematically."
      ],
      "metadata": {
        "id": "wZBFzIaZU3gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define grid search parameters\n",
        "learning_rates =\n",
        "batch_sizes =\n",
        "\n",
        "# Train and evaluate for all combinations\n",
        "best_loss = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        optimizer = # Code Here\n",
        "        train_loader = # Code Here\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Code Here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        print(f\"LR: {lr}, Batch size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_params = {'lr': lr, 'batch_size': batch_size}\n",
        "\n",
        "print(f\"Best Params (Grid Search): {best_params}\")\n"
      ],
      "metadata": {
        "id": "TcJybSHKVSu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W-oxYMQFiuwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Search:**\n",
        "\n",
        "Randomly select hyperparameters for 5 trials from:\n",
        "* Learning rate: [0.0001, 0.001, 0.01, 0.1]\n",
        "* Batch size: [16, 32, 64, 128]"
      ],
      "metadata": {
        "id": "cZBDWk_wV3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define random search space\n",
        "learning_rates =\n",
        "batch_sizes =\n",
        "# Randomly sample 5 combinations\n",
        "for _ in range(5):\n",
        "    lr = random.choice(learning_rates)\n",
        "    batch_size = random.choice(batch_sizes)\n",
        "    for batch_size in batch_sizes:\n",
        "        optimizer = # Code Here\n",
        "        train_loader = # Code Here\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Code Here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        print(f\"LR: {lr}, Batch size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_params = {'lr': lr, 'batch_size': batch_size}\n",
        "\n",
        "print(f\"Best Params (Random Search): {best_params}\")\n"
      ],
      "metadata": {
        "id": "Tpbk9NjpV5GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4ewVlxfjiv9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayesian Optimization (Optuna):**\n",
        "\n",
        "Use optuna.create_study to dynamically suggest:\n",
        "* Learning rate: Range (0.0001, 0.1)\n",
        "* Hidden layer neurons: Range (32, 256)"
      ],
      "metadata": {
        "id": "u46y5GicWU4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest parameters\n",
        "    lr = # Code Here\n",
        "    neurons = # Code Here\n",
        "\n",
        "    # Modify model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(28*28, neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(neurons, 10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "    optimizer = # Code Here\n",
        "    loss_function = # Code Here\n",
        "\n",
        "    # Train model\n",
        "    model.train()\n",
        "    num_epochs =\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, labels in train_loader:\n",
        "            # Flatten images\n",
        "            images = images.view(images.size(0), -1)\n",
        "\n",
        "            # Forward pass\n",
        "\n",
        "\n",
        "            # Backward pass\n",
        "\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # Flatten images\n",
        "            images = images.view(images.size(0), -1)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(images)\n",
        "            loss = loss_function(predictions, labels)\n",
        "\n",
        "            # Append the total_loss\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)  # Average loss over all batches\n",
        "    return avg_loss  # Return loss for Optuna to minimize\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(f\"Best Params (Optuna): {study.best_params}\")\n"
      ],
      "metadata": {
        "id": "8pX7C-hQWYzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rhLh1t7LYnmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step5**: Evaluate and Compare the Model\n",
        "\n",
        "* Train the model using the best hyperparameters from each method (Grid Search, Random Search, Optuna).\n",
        "* num_epochs = 50\n",
        "* Evaluate all models on the test set.\n",
        "* Plot training/validation accuracy and loss for the best model.\n"
      ],
      "metadata": {
        "id": "DAkXzAgHW9wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with best params and evaluate\n",
        "model = FashionNN()  # Re-initialize the model\n",
        "optimizer = # Code here\n",
        "train_loader = # Code here\n",
        "\n",
        "# Define loss function\n",
        "loss_function = # Code here\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50 # Re-train with best parameters\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    model.train()  # Set model to training mode\n",
        "    for images, labels in train_loader:\n",
        "        # Clear previous gradients\n",
        "\n",
        "        predictions =                # Forward pass\n",
        "\n",
        "        loss =                       # Compute loss\n",
        "        # Backpropagation\n",
        "\n",
        "        # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()  # Set model to evaluation mode\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "    for images, labels in test_loader:\n",
        "        predictions =            # Forward pass\n",
        "        loss =                   # Compute loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(predictions, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aMRSKltYXPTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n4LgAqQ1Yo7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step6**: Visualize the Model"
      ],
      "metadata": {
        "id": "e0sNkV7nYsZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "model.eval()\n",
        "images, labels = next(iter(test_loader))\n",
        "predictions = model(images).argmax(dim=1)\n",
        "\n",
        "# Plot 9 images\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"Pred: {predictions[i]}, True: {labels[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W1TNb1LaYwq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}