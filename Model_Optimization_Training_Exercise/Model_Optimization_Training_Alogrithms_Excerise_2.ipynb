{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: **BONUS EXERCISE**"
      ],
      "metadata": {
        "id": "HpRkAlxQwQjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and CUDA"
      ],
      "metadata": {
        "id": "Yvpy57daLojm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOiJAjpnklhJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: **Gradient Descent for Demand Forecasting at AtliQ**\n",
        "\n",
        "AtliQ wants to optimize the prediction of regional product demands using gradient descent.\n",
        "\n",
        "Assume the loss function is\n",
        "\n",
        "$$L(w)=(w‚àí4)^2$$\n",
        "\n",
        "where **w** is a weight parameter initialized at 0.\n",
        "\n",
        "**Write code to:**\n",
        "\n",
        "* Perform 10 iterations of gradient descent using a learning rate of 0.1.\n",
        "\n",
        "* Print the weight **w** at each step.\n",
        "\n"
      ],
      "metadata": {
        "id": "GyYMNU-bmZXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "w = 0.0\n",
        "\n",
        "for i in range (10):\n",
        "  gradient = # Code Here\n",
        "  w = # Code Here (Update Rule)\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ],
      "metadata": {
        "id": "bhYyNy9mndvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xADdX0ULpDhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: **Momentum for Contour Navigation in AtliQ's Supply Chain**\n",
        "\n",
        "AtliQ's supply chain optimization problem is represented by a contour map of a quadratic function:\n",
        "\n",
        "$$f(x,y)=x^2 +3y^2$$\n",
        "\n",
        "Write a code to implement gradient descent (5 iterations) with momentum to minimize this function.\n",
        "\n",
        "Use:\n",
        "* Initial point (x, y) = (2, 2)\n",
        "* Learning rate (Œ∑) = 0.1\n",
        "* Momentum Coefficient (Œ≤)) = 0.9"
      ],
      "metadata": {
        "id": "40THnZYtoOkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x, y):\n",
        "  return # Code Here (Gradients of f(x, y))\n",
        "\n",
        "x, y =\n",
        "learning_rate =\n",
        "momentum =\n",
        "vx, vy = 0.0, 0.0 # initialized velocity\n",
        "\n",
        "for i in range(5):\n",
        "  dx, dy = gradient(x, y)\n",
        "  vx = # Code Here\n",
        "  vy = # Code Here\n",
        "  x += vx\n",
        "  y += vy\n",
        "  print(f\"Step {i+1}: x = {x:.4f}, y = {y:.4f}\")"
      ],
      "metadata": {
        "id": "z2wHwhiHpCwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "t2tQRlsUqLvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: **RMS Prop for AtliQ's Dynamic Pricing Optimization**\n",
        "\n",
        "AtliQ's AI model adjusts product prices dynamically. Implement the RMSProp optimizer for minimizing the function:\n",
        "\n",
        "$$f(w) = w^2 + 5$$\n",
        "\n",
        "Use:\n",
        "\n",
        "* Initial weight (ùë§) = 5.0\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficient(Œ≤)=0.9\n",
        "\n",
        "\n",
        "Run the optimization for 15 iterations and print the weight updates."
      ],
      "metadata": {
        "id": "VJjaxm6kqNKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(w):\n",
        "  return # Code Here (Gradients of f(w))\n",
        "\n",
        "w =\n",
        "learning_rate =\n",
        "beta =\n",
        "epsilon = 1e-8\n",
        "squared_gradient_average = 0.0 # initialized squared gradient average\n",
        "\n",
        "for i in range(15):\n",
        "  grad = gradient(w)\n",
        "  squared_gradient_average = # Code Here\n",
        "  w = # Code Here (Update Rule)\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ],
      "metadata": {
        "id": "4GjcPpNKrwzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S0tb736Ysolc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4: **Adam Optimizer for AtliQ AI Models**\n",
        "\n",
        "AtliQ is training an AI model to recommend warehouse restocking schedules. Use the Adam optimizer to minimize the function:\n",
        "\n",
        "$$f(x) = x^4 - 3x^3 + 2$$\n",
        "\n",
        "Write code to:\n",
        "\n",
        "* Initialize x = 3.0\n",
        "\n",
        "Run the optimizations for 19 iterations (starting from 1) with:\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficients: Œ≤1 = 0.9, Œ≤2 = 0.09\n"
      ],
      "metadata": {
        "id": "87npAc9ysqOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x):\n",
        "  return # Code Here (Gradients of f(x))\n",
        "\n",
        "x =\n",
        "learning_rate =\n",
        "beta1, beta2 =\n",
        "epsilon =\n",
        "first_moment, second_moment = 0.0, 0.0 # initialized first and second moment\n",
        "\n",
        "for t in range(1, 20):\n",
        "  grad = gradient(x)\n",
        "  m = # Code here (update biased first moment)\n",
        "  v = # Code here (update biased second moment)\n",
        "  m_hat = # Code here (corrected first moment)\n",
        "  v_hat = # Code here (corrected second moment)\n",
        "  x = # Code here (update rule)\n",
        "  first_moment, second_moment = m, v\n",
        "  print(f\"Step {t}: x = {x:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dQjK7sR4twzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "953S3HD64qNm"
      }
    }
  ]
}