{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: **AtliQ's Customer Churn Prediction**\n",
        "\n",
        "### Welcome to AtliQ Electronics, a leading consumer electronics retailer! AtliQ has been facing customer churn issues and is building an AI-powered predictive model to identify customers likely to discontinue their services. However, the data science team observed that the model tends to overfit the training data, leading to poor generalization on new data. Your task is to implement and compare different regularization techniques to build a robust and accurate churn prediction model.\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "* Batchnorm1d (PyTorch): [Link](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n"
      ],
      "metadata": {
        "id": "jKQ4bimgnLZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and CUDA"
      ],
      "metadata": {
        "id": "nB2zACC84i0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "hv5okv7X4kiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's do some revisoin first and test your basic understanding of this module!"
      ],
      "metadata": {
        "id": "bexc1B_IaiNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem1: **Effect of L1 Regularization on Sparsity in AtliQ's AI Models**\n",
        "\n",
        "AtliQ is optimizing its AI models to reduce unnecessary complexity. You are given the following weights from one of AtliQ's neural network layers:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "weights = torch.tensor([0.5, -0.3, 0.8, -1.5], requires_grad=True)\n",
        "```\n",
        "\n",
        "Write a PyTorch snippet to compute the L1 regularization term ``` (|w1| + |w2| + ...) ``` and calculate its gradient using backward().\n"
      ],
      "metadata": {
        "id": "gmftSYXMawJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights\n",
        "weights = # Code Here\n",
        "\n",
        "# Compute L1 regularization term\n",
        "l1_regularization = torch.sum(torch.abs(weights))\n",
        "\n",
        "# compute gradients\n",
        "\n",
        "# Output gradients\n",
        "print(f\"L1 Regularization: {l1_regularization.item():.4f}\")\n",
        "print(f\"Gradients: {weights.grad}\")"
      ],
      "metadata": {
        "id": "cgXxBUt2eQ5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SBMQibCFevih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem2: **Batch Normalization of AtliQ's Marketing Data**\n",
        "\n",
        "AtliQ has collected a mini-batch of marketing data:\n",
        "\n",
        "`inputs = torch.tensor([[10.0, 20.0], [15.0, 25.0], [12.0, 18.0]])`\n",
        "\n",
        "Manually implement the forward pass of Batch Normalization. Normalize the data using:\n",
        "\n",
        "$$Normalized= X-μ / σ$$\n",
        "\n",
        "Here, μ is the mean of each feature, and σ is the standard deviation.\n",
        "\n",
        "Write code to compute the normalized data without using `nn.BatchNorm`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNIBQL9NewoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs\n",
        "inputs = # Code Here\n",
        "\n",
        "# compute mean and standard deviation along each column (feature)\n",
        "mean = # Code Here\n",
        "std = # Code Here\n",
        "\n",
        "# normalize the input\n",
        "normalized = # Code Here\n",
        "\n",
        "print(f\"Inputs:\\n{inputs}\")\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard Deviation: {std}\")\n",
        "print(f\"Normalized Outputs:\\n{normalized}\")"
      ],
      "metadata": {
        "id": "CpSgXDgzfZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3GNmDoQ6f3gJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task: **AtliQ's Customer Churn Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "CQfmadAunB37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description**\n",
        "\n",
        "You are provided with 5000 customer records and 6 features that describe customer behavior over the past 12 months. The provided dataset (**AtliQ_Churn_Prediction_Codebasics_DL.csv)** includes the following attributes:\n",
        "\n",
        "* Purchase_History\n",
        "* Support_Tickets\n",
        "* Last_Purchase_Months\n",
        "* Product_Categories\n",
        "* Satisfaction_Score\n",
        "* Discount_Usage_Rate\n",
        "\n",
        "Target Variable:\n",
        "\n",
        "* Churned: 1 if the customer has churned, 0 otherwise."
      ],
      "metadata": {
        "id": "iBmVAgUzniwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1**: Load and Prepare the Dataset"
      ],
      "metadata": {
        "id": "kwMQWwt3ohSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = # Code Here\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(\"Churned\", axis=1).values\n",
        "y = # Code Here\n",
        "\n",
        "data,info()"
      ],
      "metadata": {
        "id": "8O0bKZ5rolPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JZSypSRR4rAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2**: Split the Dataset\n",
        "\n",
        "Train : Test :: 70 : 30"
      ],
      "metadata": {
        "id": "99dir8N4ovsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Lbvye_8I4sFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = # Code Here\n"
      ],
      "metadata": {
        "id": "6RibvycYpBb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_ILShmPy4tc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3**: Normalize the features\n",
        "\n"
      ],
      "metadata": {
        "id": "RukDY2i9pNDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StadardScaler()\n",
        "X_train = # Code Here\n",
        "X_test = # Code Here"
      ],
      "metadata": {
        "id": "LDyxZDo3pSFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KyVTFdaM4um3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step4**: Convert to PyTorch Tensors"
      ],
      "metadata": {
        "id": "JxnXxajZpcQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "rBpxi2svpgvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LcLqDUMT4viD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step5**: Build a Base Model (without regularization)\n",
        "\n",
        "Details:\n",
        "* Input layer: 6 features\n",
        "* Hidden layers: Two layers with 32 and 16 neurons (ReLU activation)\n",
        "* Output layer: 1 neuron (Sigmoid activation for binary classification)\n",
        "* Train for 30 epochs, batch size = 32, learning rate = 0.01.\n",
        "* Use **BCELoss** and **SGD** as Optimizer"
      ],
      "metadata": {
        "id": "_qAlu0tApj2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "           # Code Here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = BaseModel()\n",
        "loss_function = # Code Here\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "AtZBzVDKp6L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kYK_BFqf4xoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step6**: Train the Base Model\n",
        "\n",
        "* epochs = 30\n",
        "* batch size = 32"
      ],
      "metadata": {
        "id": "1tWnv--JqwPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs =\n",
        "batch_size =\n",
        "num_batches = len(X_train_tensor) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        X_batch = X_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "        y_batch = y_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = # Code here\n",
        "        loss = # code here\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_loss_history.append(epoch_loss / num_batches)\n",
        "\n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_predictions = # Code Here\n",
        "        val_loss = # Code Here\n",
        "        val_loss_history.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/num_batches:.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.plot(range(1, epochs+1), train_loss_history, label=\"Training Loss\")\n",
        "plt.plot(range(1, epochs+1), val_loss_history, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pWC7Y0G5q2pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fOmDLuyo40km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step7**: Implement Dropout Regularization\n",
        "\n",
        "Modify the base model to include dropout layers after each dense layer.\n",
        "* Experiment with dropout rate 0.4."
      ],
      "metadata": {
        "id": "7V84oNACrIJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(6, 16),\n",
        "            nn.ReLU(),\n",
        "            # Add Dropout\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            # Add Dropout\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "dropout_model = DropoutModel()\n",
        "optimizer_dropout = optim.SGD(dropout_model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "Dvx5sbvSrar3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fh1ZHA8p42QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step8**: Training Process for Dropout Regularization"
      ],
      "metadata": {
        "id": "99hKQHYJ1Smo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for Dropout Regularization (Similar training process as above)\n",
        "epochs =\n",
        "batch_size =\n",
        "num_batches = len(X_train_tensor) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    dropout_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        X_batch = X_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "        y_batch = y_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        optimizer_dropout.zero_grad()\n",
        "        y_pred = # Code here\n",
        "        loss = # code here\n",
        "        loss.backward()\n",
        "        optimizer_dropout.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_loss_history.append(epoch_loss / num_batches)\n",
        "\n",
        "    # Validation loss\n",
        "    dropout_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_predictions = # Code Here\n",
        "        val_loss = # Code Here\n",
        "        val_loss_history.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/num_batches:.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.plot(range(1, epochs+1), train_loss_history, label=\"Training Loss\")\n",
        "plt.plot(range(1, epochs+1), val_loss_history, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QSb6ZjA31YV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jNzZBOkD451z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step9**: Apply L2 Regularization\n",
        "\n",
        "* Use `torch.optim.SGD` with `weight_decay=0.1` for L2 regularization.\n",
        "* Train the model and monitor validation loss and accuracy."
      ],
      "metadata": {
        "id": "k5WJWr332ocK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 regularization (weight decay)\n",
        "optimizer_l2 = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
        "\n",
        "# Train the model for L2 Regularization (Similar training process as above)\n",
        "epochs =\n",
        "batch_size =\n",
        "num_batches = len(X_train_tensor) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # model will remain the same\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        X_batch = X_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "        y_batch = y_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        optimizer_l2.zero_grad()\n",
        "        y_pred = # Code here\n",
        "        loss = # code here\n",
        "        loss.backward()\n",
        "        optimizer_l2.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_loss_history.append(epoch_loss / num_batches)\n",
        "\n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_predictions = # Code Here\n",
        "        val_loss = # Code Here\n",
        "        val_loss_history.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/num_batches:.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.plot(range(1, epochs+1), train_loss_history, label=\"Training Loss\")\n",
        "plt.plot(range(1, epochs+1), val_loss_history, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MJDhFWoZ1SNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1mahxoY5XT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step10**: Add Batch Normalization\n",
        "\n",
        "* use BatchNorm1d\n",
        "* use **SGD** Optimizer with learning rate = 0.01"
      ],
      "metadata": {
        "id": "MyyI37IR3OzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BatchNormModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 32),\n",
        "            # Batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            # Batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Train the BatchNormModel\n",
        "batchnorm_model = BatchNormModel()\n",
        "optimizer_bn = optim.SGD(batchnorm_model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "D6F5oopM3OP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MRn2zhZV5V3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step11**: Training Process for Batch Normalization"
      ],
      "metadata": {
        "id": "Z6kmyOba33M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for Batch Normalization (Similar training process as above)\n",
        "epochs =\n",
        "batch_size =\n",
        "num_batches = len(X_train_tensor) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    batchnorm_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        X_batch = X_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "        y_batch = y_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        optimizer_bn.zero_grad()\n",
        "        y_pred = # Code here\n",
        "        loss = # code here\n",
        "        loss.backward()\n",
        "        optimizer_bn.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_loss_history.append(epoch_loss / num_batches)\n",
        "\n",
        "    # Validation loss\n",
        "    batchnorm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_predictions = # Code Here\n",
        "        val_loss = # Code Here\n",
        "        val_loss_history.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/num_batches:.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.plot(range(1, epochs+1), train_loss_history, label=\"Training Loss\")\n",
        "plt.plot(range(1, epochs+1), val_loss_history, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZOV-DXWP4AV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GeqdHddAH2r9"
      }
    }
  ]
}